\section{Bayesian Deep Learning}
\begin{framed}
    A deep \textbf{neural network} is a function: $\vf(\vx; \vtheta) \defeq \vvarphi(\mW_L \vvarphi(\mW_{L-1} ( \cdots \vvarphi(\mW_1 \vx))))$, where $\vtheta \defeq [\mW_1, \dots, \mW_L]$ is a vector of \textbf{weights}, and $\varphi : \R \to \R$ is a component-wise nonlinear \textbf{activation function}. Examples of such: \\
    $\mathrm{Tanh}(z) \defeq \frac{\exp(z) - \exp(-z)}{\exp(z) + \exp(-z)}$ \\
    $\mathrm{ReLU}(z) \defeq \max \{z, 0\} \in [0, \infty)$.
\end{framed}
\textbf{Softmax}: $\sigma_i(\vf) \defeq \frac{\exp(f_i)}{\sum_{j=1}^c \exp(f_j)}$ (classification)
\begin{framed}
    \textbf{Bayesian neural networks}: Gaussian prior on weights $\vtheta \sim \N{\vzero}{\sigmap^2 \mI}$, and Gaussian likelihood to describe how well the data is described by the model: \\
    $y \mid \vx, \vtheta \sim \N{f(\vx; \vtheta)}{\sigman^2}$. The MAP estimate is: \\
    $\vthetahat = \argmin_\vtheta \frac{1}{2 \sigmap^2} \norm{\vtheta}_2^2 + \frac{1}{2 \sigman^2} \sum_{i=1}^n (y_i - f(\vx_i; \vtheta))^2$. Update: $\vtheta \gets \vtheta(1 - \frac{\eta_t}{\sigmap^2}) + \eta_t \sum_{i=1}^n \grad \log p(y_i \mid \vx_i, \vtheta)$
\end{framed}
\textbf{Heteroscedastic Noise}:\\ Use a neural network with 2 outputs $f_1, f_2$, and define: $y \mid \vx, \vtheta \sim \N{\mu(\vx; \vtheta)}{\sigma^2(\vx; \vtheta)}$ where $\mu(\vx; \vtheta) \defeq f_1(\vx; \vtheta)$ and $\sigma^2(\vx; \vtheta) \defeq \exp(f_2(\vx; \vtheta))$.
$\log p(y_i \mid \vx_i, \vtheta) = \const - \frac{1}{2} \brackets*{\log \sigma^2(\vx_i; \vtheta) + \frac{(y_i - \mu(\vx_i; \vtheta))^2}{\sigma^2(\vx_i; \vtheta)}}$.
Approximate predictive distribution by sampling from the variational posterior $p(\ys \mid \vxs, \vx_{1:n}, \vy_{1:n}) \approx \E[\theta \sim q_\vlambda]{p(\ys \mid \vxs, \vtheta)}\approx \frac{1}{m} \sum_{i=1}^m p(\ys \mid \vxs, \vtheta^{(i)})$.